import os
from fastapi import FastAPI
from llama_cpp import Llama

app = FastAPI()

model_path = os.path.abspath("python_service\models\llama-2-7b.Q4_K_M.gguf")
print("âœ… Model path:", model_path)
assert os.path.exists(model_path), f"Le modÃ¨le n'existe pas Ã  {model_path}"

llm = Llama(
    model_path=model_path,
    n_ctx=1024,
    n_threads=6,
    n_batch=128,
    verbose=True  # ðŸ‘€ pour voir les logs
)

@app.post("/raw")
def raw_output():
    prompt = (
        "Tu es un modÃ¨le de test. RÃ©ponds librement Ã  ce prompt : "
        "Invente une Ã©nigme courte et logique avec sa rÃ©ponse et un indice."
    )

    output = llm(
        prompt,
        max_tokens=300,
        temperature=0.7,
        top_p=0.9,
        repeat_penalty=1.1
    )

    text = output["choices"][0]["text"].strip()
    print("\nðŸ§  Sortie brute du modÃ¨le :\n", text, "\n")
    return {"raw_output": text}
